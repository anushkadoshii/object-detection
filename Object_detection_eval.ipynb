{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka8T9ybzaqFD",
        "outputId": "c81017e7-0dd4-4e2d-81d3-1feec8dde733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading COCO validation images (5000 images)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "val2017.zip: 100%|██████████| 778M/778M [00:18<00:00, 44.0MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading COCO annotations...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "annotations_trainval2017.zip: 100%|██████████| 241M/241M [00:05<00:00, 49.2MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting validation images...\n",
            "Extracting annotations...\n",
            "COCO validation set downloaded successfully!\n",
            "Found 5000 images in ./coco_data/val2017\n",
            "Annotations file contains information for 5000 images and 36781 annotations\n",
            "Dataset verification completed successfully!\n",
            "\n",
            "Dataset information:\n",
            "- Images directory: ./coco_data/val2017\n",
            "- Annotations file: ./coco_data/annotations/instances_val2017.json\n",
            "\n",
            "The COCO validation set (5000 images) with annotations is ready for evaluation!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "def download_file(url, destination):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024  # 1 Kibibyte\n",
        "\n",
        "    with open(destination, 'wb') as file, tqdm(\n",
        "            desc=os.path.basename(destination),\n",
        "            total=total_size,\n",
        "            unit='iB',\n",
        "            unit_scale=True,\n",
        "            unit_divisor=1024,\n",
        "        ) as bar:\n",
        "        for data in response.iter_content(block_size):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)\n",
        "\n",
        "def download_coco_validation_set(root_dir='./coco_data'):\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "    # Check if already downloaded\n",
        "    if os.path.exists(os.path.join(root_dir, 'val2017')) and \\\n",
        "       os.path.exists(os.path.join(root_dir, 'annotations')):\n",
        "        print(\"COCO validation set already exists!\")\n",
        "        return os.path.join(root_dir, 'val2017'), os.path.join(root_dir, 'annotations/instances_val2017.json')\n",
        "\n",
        "    \n",
        "    val_images_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
        "    annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "\n",
        "  \n",
        "    print(\"Downloading COCO validation images (5000 images)...\")\n",
        "    val_zip_path = os.path.join(root_dir, \"val2017.zip\")\n",
        "    download_file(val_images_url, val_zip_path)\n",
        "\n",
        "    \n",
        "    print(\"ddownloading COCO annotations...\")\n",
        "    ann_zip_path = os.path.join(root_dir, \"annotations_trainval2017.zip\")\n",
        "    download_file(annotations_url, ann_zip_path)\n",
        "\n",
        "    # Extract validation images\n",
        "    print(\"extracting validation images...\")\n",
        "    with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(root_dir)\n",
        "\n",
        "    \n",
        "    print(\"extracting annotations...\")\n",
        "    with zipfile.ZipFile(ann_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(root_dir)\n",
        "\n",
        "    # Clean up zip files\n",
        "    os.remove(val_zip_path)\n",
        "    os.remove(ann_zip_path)\n",
        "\n",
        "    print(\"COCO validation set downloaded successfully!\")\n",
        "    return os.path.join(root_dir, 'val2017'), os.path.join(root_dir, 'annotations/instances_val2017.json')\n",
        "\n",
        "def verify_coco_dataset(images_dir, annotations_file):\n",
        "    # Check if images directory exists and has files\n",
        "    if not os.path.exists(images_dir):\n",
        "        print(f\"Error: Images directory {images_dir} does not exist\")\n",
        "        return False\n",
        "\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
        "    print(f\"Found {len(image_files)} images in {images_dir}\")\n",
        "\n",
        "    # Check if annotations file exists and is valid JSON\n",
        "    if not os.path.exists(annotations_file):\n",
        "        print(f\"Error: Annotations file {annotations_file} does not exist\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        with open(annotations_file, 'r') as f:\n",
        "            annotations = json.load(f)\n",
        "            num_images = len(annotations['images'])\n",
        "            num_annotations = len(annotations['annotations'])\n",
        "            print(f\"annotations file contains information for {num_images} images and {num_annotations} annotations\")\n",
        "    except Exception as e:\n",
        "        print(f\"error reading annotations file: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(\"dataset verification completed successfully!\")\n",
        "    return True\n",
        "\n",
        "images_dir, annotations_file = download_coco_validation_set()\n",
        "\n",
        "# Verify the dataset\n",
        "verify_coco_dataset(images_dir, annotations_file)\n",
        "\n",
        "print(\"\\nDataset information:\")\n",
        "print(f\"- Images directory: {images_dir}\")\n",
        "print(f\"- Annotations file: {annotations_file}\")\n",
        "print(\"\\nThe COCO validation set (5000 images) with annotations is ready for evaluation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zED2hVsav6c",
        "outputId": "e50543a1-00db-4001-daa8-7e5452c8893c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.73s)\n",
            "creating index...\n",
            "index created!\n",
            "Created data loader with 5000 images for evaluation\n",
            "Number of images: 5000\n",
            "Number of categories: 80\n",
            "\n",
            "Some COCO categories:\n",
            "  1. person\n",
            "  2. bicycle\n",
            "  3. car\n",
            "  4. motorcycle\n",
            "  5. airplane\n",
            "  6. bus\n",
            "  7. train\n",
            "  8. truck\n",
            "  9. boat\n",
            "  10. traffic light\n",
            "  11. fire hydrant\n",
            "  12. stop sign\n",
            "  13. parking meter\n",
            "  14. bench\n",
            "  15. bird\n",
            "  16. cat\n",
            "  17. dog\n",
            "  18. horse\n",
            "  19. sheep\n",
            "  20. cow\n",
            "  21. elephant\n",
            "  22. bear\n",
            "  23. zebra\n",
            "  24. giraffe\n",
            "  25. backpack\n",
            "  26. umbrella\n",
            "  27. handbag\n",
            "  28. tie\n",
            "  29. suitcase\n",
            "  30. frisbee\n",
            "  31. skis\n",
            "  32. snowboard\n",
            "  33. sports ball\n",
            "  34. kite\n",
            "  35. baseball bat\n",
            "  36. baseball glove\n",
            "  37. skateboard\n",
            "  38. surfboard\n",
            "  39. tennis racket\n",
            "  40. bottle\n",
            "  41. wine glass\n",
            "  42. cup\n",
            "  43. fork\n",
            "  44. knife\n",
            "  45. spoon\n",
            "  46. bowl\n",
            "  47. banana\n",
            "  48. apple\n",
            "  49. sandwich\n",
            "  50. orange\n",
            "  51. broccoli\n",
            "  52. carrot\n",
            "  53. hot dog\n",
            "  54. pizza\n",
            "  55. donut\n",
            "  56. cake\n",
            "  57. chair\n",
            "  58. couch\n",
            "  59. potted plant\n",
            "  60. bed\n",
            "  61. dining table\n",
            "  62. toilet\n",
            "  63. tv\n",
            "  64. laptop\n",
            "  65. mouse\n",
            "  66. remote\n",
            "  67. keyboard\n",
            "  68. cell phone\n",
            "  69. microwave\n",
            "  70. oven\n",
            "  71. toaster\n",
            "  72. sink\n",
            "  73. refrigerator\n",
            "  74. book\n",
            "  75. clock\n",
            "  76. vase\n",
            "  77. scissors\n",
            "  78. teddy bear\n",
            "  79. hair drier\n",
            "  80. toothbrush\n",
            "\n",
            "Dataset is ready for evaluation!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class COCOEvalDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, root, annFile,transform=None):\n",
        "        self.root = root\n",
        "        self.coco = COCO(annFile)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "        # Get categories\n",
        "        self.categories = self.coco.loadCats(self.coco.getCatIds())\n",
        "        self.categories.sort(key=lambda x: x['id'])\n",
        "        self.category_ids = [category['id'] for category in self.categories]\n",
        "        self.category_names = [category['name'] for category in self.categories]\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_id = self.ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root, img_info['file_name'])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations that match the pretrained models\n",
        "        transform = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            # T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        image = transform(img)\n",
        "\n",
        "        # Get annotations\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        areas = []\n",
        "        iscrowd = []\n",
        "\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann['bbox']\n",
        "            # Convert to [x1, y1, x2, y2] format\n",
        "            boxes.append([x, y, (x + w), (y + h)])\n",
        "\n",
        "            # Map COCO category_id to model index\n",
        "            # Handle case where category might not be in our mapping\n",
        "            cat_id = ann['category_id']\n",
        "            if cat_id in self.category_ids:\n",
        "                labels.append(cat_id)\n",
        "            else:\n",
        "                # Skip annotations with unknown categories\n",
        "                continue\n",
        "\n",
        "            areas.append(ann['area'])\n",
        "            iscrowd.append(ann['iscrowd'])\n",
        "\n",
        "        # Convert to tensors\n",
        "        if boxes:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "            iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "        else:\n",
        "            # Handle images with no annotations\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "            areas = torch.zeros((0,), dtype=torch.float32)\n",
        "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        # Create target dict\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([img_id]),\n",
        "            'area': areas,\n",
        "            'iscrowd': iscrowd\n",
        "        }\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def create_coco_loader(images_dir, annotations_file, batch_size=1, num_workers=4):\n",
        "    # Define transforms\n",
        "    transform = T.Compose([\n",
        "        # T.Resize((300, 300)),  # Resize to 300x300\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = COCOEvalDataset(\n",
        "        root=images_dir,\n",
        "        annFile=annotations_file,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Create data loader\n",
        "\n",
        "\n",
        "    print(f\"Created data loader with {len(dataset)} images for evaluation\")\n",
        "    return dataset\n",
        "\n",
        "images_dir = \"./coco_data/val2017\"\n",
        "annotations_file = \"./coco_data/annotations/instances_val2017.json\"\n",
        "\n",
        "# Create data loader\n",
        "loader, dataset = create_coco_loader(images_dir, annotations_file)\n",
        "\n",
        "# Print some dataset statistics\n",
        "print(f\"Number of images: {len(dataset)}\")\n",
        "print(f\"Number of categories: {len(dataset.categories)}\")\n",
        "\n",
        "# Display some category names\n",
        "print(\"\\nSome COCO categories:\")\n",
        "for i, name in enumerate(dataset.category_names):\n",
        "    print(f\"  {i+1}. {name}\")\n",
        "\n",
        "print(\"\\nDataset is ready for evaluation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3rBCyN8a4Dh",
        "outputId": "4f6ebfcd-2492-46c8-9fd1-a038e7efce7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth\n",
            "100%|██████████| 146M/146M [00:00<00:00, 170MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.75s)\n",
            "creating index...\n",
            "index created!\n",
            "Created data loader with 5000 images for evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved detection visualizations to coco_detection_examples.png\n",
            "Running inference...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [10:37<00:00,  7.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running custom evaluation...\n",
            "\n",
            "Evaluation Results:\n",
            "mAP @ IoU=0.5: 0.5978\n",
            "\n",
            "AP by category:\n",
            "Category 1: 0.7571\n",
            "Category 2: 0.5649\n",
            "Category 3: 0.6503\n",
            "Category 4: 0.6962\n",
            "Category 5: 0.8572\n",
            "Category 6: 0.8100\n",
            "Category 7: 0.8342\n",
            "Category 8: 0.5587\n",
            "Category 9: 0.5150\n",
            "Category 10: 0.5399\n",
            "Category 11: 0.8495\n",
            "Category 13: 0.7079\n",
            "Category 14: 0.6213\n",
            "Category 15: 0.3814\n",
            "Category 16: 0.5174\n",
            "Category 17: 0.8812\n",
            "Category 18: 0.8078\n",
            "Category 19: 0.7882\n",
            "Category 20: 0.7620\n",
            "Category 21: 0.7566\n",
            "Category 22: 0.8267\n",
            "Category 23: 0.8949\n",
            "Category 24: 0.8615\n",
            "Category 25: 0.8691\n",
            "Category 27: 0.3226\n",
            "Category 28: 0.6113\n",
            "Category 31: 0.3025\n",
            "Category 32: 0.5225\n",
            "Category 33: 0.5877\n",
            "Category 34: 0.8174\n",
            "Category 35: 0.4750\n",
            "Category 36: 0.5419\n",
            "Category 37: 0.6090\n",
            "Category 38: 0.5920\n",
            "Category 39: 0.5602\n",
            "Category 40: 0.6384\n",
            "Category 41: 0.7810\n",
            "Category 42: 0.5864\n",
            "Category 43: 0.7733\n",
            "Category 44: 0.5520\n",
            "Category 46: 0.5700\n",
            "Category 47: 0.5830\n",
            "Category 48: 0.5067\n",
            "Category 49: 0.3002\n",
            "Category 50: 0.3033\n",
            "Category 51: 0.5643\n",
            "Category 52: 0.3863\n",
            "Category 53: 0.3324\n",
            "Category 54: 0.5415\n",
            "Category 55: 0.4087\n",
            "Category 56: 0.4217\n",
            "Category 57: 0.3523\n",
            "Category 58: 0.4943\n",
            "Category 59: 0.6799\n",
            "Category 60: 0.5942\n",
            "Category 61: 0.5232\n",
            "Category 62: 0.4534\n",
            "Category 63: 0.6071\n",
            "Category 64: 0.4572\n",
            "Category 65: 0.6414\n",
            "Category 67: 0.4340\n",
            "Category 70: 0.7619\n",
            "Category 72: 0.7431\n",
            "Category 73: 0.7512\n",
            "Category 74: 0.7827\n",
            "Category 75: 0.5012\n",
            "Category 76: 0.6839\n",
            "Category 77: 0.5597\n",
            "Category 78: 0.7621\n",
            "Category 79: 0.5276\n",
            "Category 80: 0.5013\n",
            "Category 81: 0.5869\n",
            "Category 82: 0.6983\n",
            "Category 84: 0.2711\n",
            "Category 85: 0.7691\n",
            "Category 86: 0.5613\n",
            "Category 87: 0.4255\n",
            "Category 88: 0.6676\n",
            "Category 89: 0.1550\n",
            "Category 90: 0.3764\n",
            "Saved precision-recall curves to 'precision_recall_curves.png'\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,FasterRCNN_MobileNet_V3_Large_FPN_Weights,retinanet_resnet50_fpn_v2,RetinaNet_ResNet50_FPN_V2_Weights\n",
        "\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision.ops import nms\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calculate IoU between two bounding boxes\n",
        "    Box format: [x1, y1, x2, y2]\n",
        "    \"\"\"\n",
        "    # Calculate intersection area\n",
        "    x_left = max(box1[0], box2[0])\n",
        "    y_top = max(box1[1], box2[1])\n",
        "    x_right = min(box1[2], box2[2])\n",
        "    y_bottom = min(box1[3], box2[3])\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "\n",
        "    # Calculate union area\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "    return intersection_area / union_area if union_area > 0 else 0.0\n",
        "\n",
        "def convert_to_xywh(box):\n",
        "    return [box[0], box[1], box[2] - box[0], box[3] - box[1]]\n",
        "\n",
        "def convert_to_xyxy(box):\n",
        "    return [box[0], box[1], box[0] + box[2], box[1] + box[3]]\n",
        "\n",
        "def compute_ap(precision, recall):\n",
        "    \"\"\"Compute Average Precision using 11-point interpolation\"\"\"\n",
        "    ap = 0.0\n",
        "    for t in np.arange(0.0, 1.1, 0.1):\n",
        "        if np.sum(recall >= t) == 0:\n",
        "            p = 0\n",
        "        else:\n",
        "            p = np.max(precision[recall >= t])\n",
        "        ap += p / 11.0\n",
        "    return ap\n",
        "\n",
        "def evaluate_detections(predictions, ground_truth, iou_threshold=0.5, max_dets=100):\n",
        "    # Group predictions and ground truth by image_id and category_id\n",
        "    pred_by_img = defaultdict(list)\n",
        "    gt_by_img = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    # Get all category ids\n",
        "    category_ids = set()\n",
        "\n",
        "    # Process predictions\n",
        "    for pred in predictions:\n",
        "        img_id = pred['image_id']\n",
        "        cat_id = pred['category_id']\n",
        "        pred_by_img[img_id].append(pred)\n",
        "        category_ids.add(cat_id)\n",
        "\n",
        "    # Process ground truth\n",
        "    for gt in ground_truth:\n",
        "        img_id = gt['image_id']\n",
        "        cat_id = gt['category_id']\n",
        "        gt_by_img[img_id][cat_id].append(gt)\n",
        "        category_ids.add(cat_id)\n",
        "\n",
        "    # Metrics storage\n",
        "    metrics = {\n",
        "        'precision': {},\n",
        "        'recall': {},\n",
        "        'ap': {},\n",
        "        'f1_score': {}\n",
        "    }\n",
        "\n",
        "    # Calculate metrics for each category\n",
        "    for cat_id in category_ids:\n",
        "        true_positives = []\n",
        "        false_positives = []\n",
        "        scores = []\n",
        "        num_gt = 0\n",
        "\n",
        "        # Count total ground truth for this category\n",
        "        for img_id in gt_by_img:\n",
        "            num_gt += len(gt_by_img[img_id].get(cat_id, []))\n",
        "\n",
        "        # Process each image\n",
        "        for img_id in pred_by_img:\n",
        "            # Get predictions for this image\n",
        "            img_preds = [p for p in pred_by_img[img_id] if p['category_id'] == cat_id]\n",
        "\n",
        "            # Sort predictions by score in descending order\n",
        "            img_preds = sorted(img_preds, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "            # Limit number of detections\n",
        "            img_preds = img_preds[:max_dets]\n",
        "\n",
        "            # Get ground truth for this image and category\n",
        "            img_gts = gt_by_img[img_id].get(cat_id, [])\n",
        "\n",
        "            # Mark each ground truth as matched or not\n",
        "            matched_gt = [False] * len(img_gts)\n",
        "\n",
        "            # Check each prediction\n",
        "            for pred in img_preds:\n",
        "                pred_bbox = convert_to_xyxy(pred['bbox'])\n",
        "                pred_score = pred['score']\n",
        "\n",
        "                best_iou = 0\n",
        "                best_gt_idx = -1\n",
        "\n",
        "                # Find best matching ground truth\n",
        "                for gt_idx, gt in enumerate(img_gts):\n",
        "                    if matched_gt[gt_idx]:\n",
        "                        continue\n",
        "\n",
        "                    gt_bbox = convert_to_xyxy(gt['bbox'])\n",
        "                    iou = calculate_iou(pred_bbox, gt_bbox)\n",
        "\n",
        "                    if iou > best_iou:\n",
        "                        best_iou = iou\n",
        "                        best_gt_idx = gt_idx\n",
        "\n",
        "                # Check if we have a match\n",
        "                if best_iou >= iou_threshold and best_gt_idx >= 0:\n",
        "                    true_positives.append(1)\n",
        "                    false_positives.append(0)\n",
        "                    matched_gt[best_gt_idx] = True\n",
        "                else:\n",
        "                    true_positives.append(0)\n",
        "                    false_positives.append(1)\n",
        "\n",
        "                scores.append(pred_score)\n",
        "\n",
        "        # Sort by score\n",
        "        inds = np.argsort(scores)[::-1]\n",
        "        true_positives = np.array(true_positives)[inds]\n",
        "        false_positives = np.array(false_positives)[inds]\n",
        "\n",
        "        # Compute cumulative sum\n",
        "        tp_cumsum = np.cumsum(true_positives)\n",
        "        fp_cumsum = np.cumsum(false_positives)\n",
        "\n",
        "        # Compute precision and recall\n",
        "        precision = tp_cumsum / (tp_cumsum + fp_cumsum + np.finfo(float).eps)\n",
        "        recall = tp_cumsum / (num_gt + np.finfo(float).eps)\n",
        "\n",
        "        # Compute AP\n",
        "        ap = compute_ap(precision, recall)\n",
        "\n",
        "        # Store metrics\n",
        "        metrics['precision'][cat_id] = precision\n",
        "        metrics['recall'][cat_id] = recall\n",
        "        metrics['ap'][cat_id] = ap\n",
        "\n",
        "        # Compute F1 score (optional)\n",
        "        if len(precision) > 0 and len(recall) > 0:\n",
        "            f1 = 2 * precision * recall / (precision + recall + np.finfo(float).eps)\n",
        "            metrics['f1_score'][cat_id] = np.max(f1)\n",
        "        else:\n",
        "            metrics['f1_score'][cat_id] = 0.0\n",
        "\n",
        "    # Compute mAP\n",
        "    metrics['mAP'] = np.mean([metrics['ap'][cat_id] for cat_id in metrics['ap']])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def main():\n",
        "    # Initialize model\n",
        "\n",
        "    # model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
        "    # weights= FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1\n",
        "    # weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "    # model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
        "    model=retinanet_resnet50_fpn_v2(weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1)\n",
        "    weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    # Initialize preprocessing\n",
        "    preprocess = weights.transforms()\n",
        "\n",
        "    # Load COCO dataset\n",
        "    images_dir = \"./coco_data/val2017\"\n",
        "    annotations_file = \"./coco_data/annotations/instances_val2017.json\"\n",
        "    dataset = create_coco_loader(images_dir, annotations_file)\n",
        "\n",
        "    # Run inference\n",
        "    results = []\n",
        "    ground_truth = []\n",
        "    img,target=dataset[0]\n",
        "    visualize_detections(model,dataset)\n",
        "\n",
        "    print(\"Running inference...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (img, target) in enumerate(tqdm(dataset)):\n",
        "            if i >= 5000:  # Limit to first 100 images for testing\n",
        "                break\n",
        "\n",
        "            image_id = target['image_id'].item()\n",
        "\n",
        "            # Add ground truth for this image\n",
        "            for obj_idx in range(len(target['boxes'])):\n",
        "                gt = {\n",
        "                    'image_id': image_id,\n",
        "                    'category_id': target['labels'][obj_idx].item(),\n",
        "                    'bbox': convert_to_xywh(target['boxes'][obj_idx].tolist()),\n",
        "                    'iscrowd': 0\n",
        "                }\n",
        "                ground_truth.append(gt)\n",
        "\n",
        "            # Run model\n",
        "            batch = [preprocess(img.to('cuda'))]\n",
        "            prediction = model(batch)[0]\n",
        "\n",
        "            # Extract predictions\n",
        "            boxes = prediction['boxes'].cpu().numpy()\n",
        "            scores = prediction['scores'].cpu().numpy()\n",
        "            labels = prediction['labels'].cpu().numpy()\n",
        "\n",
        "            # Convert predictions to COCO format\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                if score >= 0.05:  # Minimum score threshold\n",
        "                    # print(score)\n",
        "                    x1, y1, x2, y2 = box.tolist()\n",
        "                    coco_box = [x1, y1, x2 - x1, y2 - y1]  # [x, y, width, height]\n",
        "\n",
        "                    category_id = label.item()\n",
        "\n",
        "                    results.append({\n",
        "                        'image_id': image_id,\n",
        "                        'category_id': category_id,\n",
        "                        'bbox': coco_box,\n",
        "                        'score': float(score)\n",
        "                    })\n",
        "\n",
        "\n",
        "    # Save results to file\n",
        "    with open('eval_results.json', 'w') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    # Run custom evaluation\n",
        "    print(\"Running custom evaluation...\")\n",
        "    metrics = evaluate_detections(results, ground_truth)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(f\"mAP @ IoU={0.5}: {metrics['mAP']:.4f}\")\n",
        "\n",
        "    # Print AP for each category\n",
        "    print(\"\\nAP by category:\")\n",
        "    for cat_id in sorted(metrics['ap'].keys()):\n",
        "        print(f\"Category {cat_id}: {metrics['ap'][cat_id]:.4f}\")\n",
        "\n",
        "    # Optional: Plot precision-recall curves\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Plot precision-recall curve for first 5 categories\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        for i, cat_id in enumerate(sorted(list(metrics['precision'].keys()))[:5]):\n",
        "            if len(metrics['precision'][cat_id]) > 0 and len(metrics['recall'][cat_id]) > 0:\n",
        "                plt.plot(\n",
        "                    metrics['recall'][cat_id],\n",
        "                    metrics['precision'][cat_id],\n",
        "                    label=f'Category {cat_id} (AP: {metrics[\"ap\"][cat_id]:.4f})'\n",
        "                )\n",
        "\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curves')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.savefig('precision_recall_curves.png')\n",
        "        plt.close()\n",
        "        print(\"Saved precision-recall curves to 'precision_recall_curves.png'\")\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib not installed. Skipping precision-recall curve plotting.\")\n",
        "\n",
        "\n",
        "\n",
        "def visualize_detections(model, dataset, n_examples=5, confidence_threshold=0.5):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    indices = np.random.choice(len(dataset), n_examples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, n_examples * 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, target = dataset[idx]\n",
        "\n",
        "        # Create a copy of the image for drawing\n",
        "        img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "        # Denormalize\n",
        "        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "        # Plot the image\n",
        "        plt.subplot(n_examples, 1, i + 1)\n",
        "        plt.imshow(img_np)\n",
        "\n",
        "\n",
        "        for box, label in zip(target['boxes'], target['labels']):\n",
        "            x, y, w, h = box[0].item(), box[1].item(), box[2].item() - box[0].item(), box[3].item() - box[1].item()\n",
        "            rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='g', facecolor='none')\n",
        "            plt.gca().add_patch(rect)\n",
        "            category_id = label.item()\n",
        "            category_name = next(cat['name'] for cat in dataset.categories if cat['id'] == category_id)\n",
        "            plt.text(x, y-5, category_name, color='g', fontsize=10, backgroundcolor='w')\n",
        "\n",
        "\n",
        "        for box, label, score in zip(prediction['boxes'], prediction['labels'], prediction['scores']):\n",
        "            if score >= confidence_threshold:\n",
        "                x, y, w, h = box[0].item(), box[1].item(), box[2].item() - box[0].item(), box[3].item() - box[1].item()\n",
        "                rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "                plt.gca().add_patch(rect)\n",
        "\n",
        "\n",
        "                idx_to_cat = {v: k for k, v in dataset.category_id_to_idx.items()}\n",
        "                category_id = label.item()\n",
        "                category_name = next((cat['name'] for cat in dataset.categories if cat['id'] == category_id), \"unknown\")\n",
        "\n",
        "                plt.text(x, y+h+15, f\"{category_name}: {score:.2f}\", color='r', fontsize=10, backgroundcolor='w')\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Image {target['image_id'].item()}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('coco_detection_examples.png')\n",
        "    plt.close()\n",
        "    print(f\"Saved detection visualizations to coco_detection_examples.png\")\n",
        "\n",
        "torch.set_default_device('cuda')\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import requests\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if not IN_COLAB:\n",
        "    print(\"This script is designed to run in Google Colab\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "!pip install -q pycocotools\n",
        "!pip install -q tqdm\n",
        "\n",
        "\n",
        "!git clone https://github.com/AlexeyAB/darknet\n",
        "\n",
        "\n",
        "!git clone https://github.com/WongKinYiu/PyTorch_YOLOv4.git\n",
        "\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights -P /content/PyTorch_YOLOv4/weights/\n",
        "\n",
        "\n",
        "DOWNLOAD_FULL_COCO = False  \n",
        "\n",
        "if DOWNLOAD_FULL_COCO:\n",
        "\n",
        "    !wget http://images.cocodataset.org/zips/val2017.zip -P /content/\n",
        "    !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P /content/\n",
        "\n",
        "    !unzip -q /content/val2017.zip -d /content/coco/\n",
        "    !unzip -q /content/annotations_trainval2017.zip -d /content/coco/\n",
        "else:\n",
        "    !mkdir -p /content/coco/val2017\n",
        "    !mkdir -p /content/coco/annotations\n",
        "    \n",
        "   \n",
        "    for i in tqdm(range(1, 101)):\n",
        "        # COCO val2017 images start from 000000000139.jpg\n",
        "        img_id = 139 + i\n",
        "        img_url = f\"http://images.cocodataset.org/val2017/{img_id:012d}.jpg\"\n",
        "        img_path = f\"/content/coco/val2017/{img_id:012d}.jpg\"\n",
        "        try:\n",
        "            response = requests.get(img_url)\n",
        "            if response.status_code == 200:\n",
        "                with open(img_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download image {img_id}: {e}\")\n",
        "    \n",
        "    # Download annotations\n",
        "    \n",
        "    !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P /content/\n",
        "    !unzip -q /content/annotations_trainval2017.zip -d /content/coco/\n",
        "\n",
        "# Change to the PyTorch_YOLOv4 directory\n",
        "os.chdir('/content/PyTorch_YOLOv4')\n",
        "\n",
        "# Import YOLOv4 modules\n",
        "sys.path.append('/content/PyTorch_YOLOv4')\n",
        "from models.models import Darknet\n",
        "from utils.datasets import LoadImagesAndLabels\n",
        "from utils.utils import non_max_suppression, xywh2xyxy, box_iou, ap_per_class, compute_ap\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "def coco80_to_coco91_class():\n",
        "    # Converts 80-index (YOLO) to 91-index (COCO)\n",
        "    return [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33,\n",
        "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61,\n",
        "            62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n",
        "\n",
        "class CocoDataset(torch.utils.data.Dataset):\n",
        "   \n",
        "    def __init__(self, img_dir, ann_file, img_size=416, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.coco = COCO(ann_file)\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "        \n",
        "       \n",
        "        ids_with_ann = []\n",
        "        for img_id in self.ids:\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "            if len(ann_ids) > 0:\n",
        "                ids_with_ann.append(img_id)\n",
        "        self.ids = ids_with_ann\n",
        "        \n",
        "        \n",
        "        self.class_names = [cat['name'] for cat in self.coco.loadCats(self.coco.getCatIds())]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        img_id = self.ids[index]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
        "        \n",
        "        \n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        \n",
        "        height, width = img.shape[:2]\n",
        "        \n",
        "        \n",
        "        img_resized = cv2.resize(img, (self.img_size, self.img_size))\n",
        "        \n",
        "        \n",
        "        img_resized = img_resized.transpose(2, 0, 1) / 255.0\n",
        "        img_resized = np.ascontiguousarray(img_resized, dtype=np.float32)\n",
        "        \n",
        "        \n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "        \n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        \n",
        "        for ann in anns:\n",
        "            if ann['area'] > 0 and ann['iscrowd'] == 0:\n",
        "                \n",
        "                x, y, w, h = ann['bbox']\n",
        "               \n",
        "                x_center = (x + w/2) / width\n",
        "                y_center = (y + h/2) / height\n",
        "                w = w / width\n",
        "                h = h / height\n",
        "                \n",
        "               \n",
        "                cat_id = ann['category_id']\n",
        "                cls_id = self.coco.getCatIds().index(cat_id)\n",
        "                \n",
        "                boxes.append([x_center, y_center, w, h])\n",
        "                labels.append(cls_id)\n",
        "        \n",
        "        if len(boxes) == 0:\n",
        "            \n",
        "            boxes = np.zeros((0, 4), dtype=np.float32)\n",
        "            labels = np.zeros(0, dtype=np.int64)\n",
        "        else:\n",
        "            boxes = np.array(boxes, dtype=np.float32)\n",
        "            labels = np.array(labels, dtype=np.int64)\n",
        "            \n",
        "        \n",
        "        targets = np.hstack((labels.reshape(-1, 1), boxes))\n",
        "        \n",
        "        return torch.from_numpy(img_resized), torch.from_numpy(targets), img_path, (height, width)\n",
        "\n",
        "def evaluate(model, dataloader, device, conf_thres=0.001, iou_thres=0.65):\n",
        "    model.eval()\n",
        "    \n",
        "    coco91class = coco80_to_coco91_class()\n",
        "    \n",
        "    # Initialize COCO json dictionaries\n",
        "    jdict = []\n",
        "    \n",
        "    for batch_i, (imgs, targets, paths, shapes) in enumerate(tqdm(dataloader)):\n",
        "        imgs = imgs.to(device).float()  # already normalized 0-1\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        # Disable gradients\n",
        "        with torch.no_grad():\n",
        "            # Run model\n",
        "            pred = model(imgs)  # inference\n",
        "            \n",
        "            # Run NMS\n",
        "            pred = non_max_suppression(pred, conf_thres=conf_thres, iou_thres=iou_thres)\n",
        "        \n",
        "        # Process detections\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            if det is None or len(det) == 0:\n",
        "                continue\n",
        "                \n",
        "            # Rescale boxes from img_size to original size\n",
        "            det[:, :4] = scale_coords(imgs.shape[2:], det[:, :4], shapes[i]).round()\n",
        "            \n",
        "            # Get image id\n",
        "            img_path = paths[i]\n",
        "            image_id = int(os.path.basename(img_path).split('.')[0])\n",
        "            \n",
        "            # Append to COCO JSON dictionary\n",
        "            for *xyxy, conf, cls in det:\n",
        "                x1, y1, x2, y2 = [float(x) for x in xyxy]\n",
        "                # Convert to COCO format: [x,y,w,h]\n",
        "                w = x2 - x1\n",
        "                h = y2 - y1\n",
        "                \n",
        "                jdict.append({\n",
        "                    'image_id': image_id,\n",
        "                    'category_id': coco91class[int(cls)],  # Convert to COCO category id\n",
        "                    'bbox': [x1, y1, w, h],\n",
        "                    'score': float(conf)\n",
        "                })\n",
        "    \n",
        "    # Save JSON\n",
        "    results_file = '/content/drive/MyDrive/results.json' if os.path.exists('/content/drive/MyDrive') else '/content/results.json'\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump(jdict, f)\n",
        "    \n",
        "    print(f\"Results saved to {results_file}\")\n",
        "    \n",
        "    # Load COCO dataset\n",
        "    coco_gt = COCO('/content/coco/annotations/instances_val2017.json')\n",
        "    coco_dt = coco_gt.loadRes(results_file)\n",
        "    \n",
        "    # Run COCO evaluation\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "    if not DOWNLOAD_FULL_COCO:\n",
        "        # If using subset, set the images to evaluate on\n",
        "        img_ids = [int(os.path.basename(p).split('.')[0]) for p in glob.glob('/content/coco/val2017/*.jpg')]\n",
        "        coco_eval.params.imgIds = img_ids\n",
        "    \n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "    \n",
        "    # Return mAP\n",
        "    return coco_eval.stats\n",
        "\n",
        "def scale_coords(img_size, coords, img_shape):\n",
        "\n",
        "    gain = min(img_size[0] / img_shape[0], img_size[1] / img_shape[1])  # gain = old / new\n",
        "    pad = (img_size[1] - img_shape[1] * gain) / 2, (img_size[0] - img_shape[0] * gain) / 2  # wh padding\n",
        "    \n",
        "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
        "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
        "    coords[:, :4] /= gain\n",
        "    \n",
        "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
        "    coords[:, 0].clamp_(0, img_shape[1])  # x1\n",
        "    coords[:, 1].clamp_(0, img_shape[0])  # y1\n",
        "    coords[:, 2].clamp_(0, img_shape[1])  # x2\n",
        "    coords[:, 3].clamp_(0, img_shape[0])  # y2\n",
        "    \n",
        "    return coords\n",
        "\n",
        "def load_model(weights_path, cfg_path, img_size=416, device='cuda'):\n",
        "\n",
        "    # Initialize model\n",
        "    model = Darknet(cfg_path, img_size)\n",
        "    \n",
        "    # Load weights\n",
        "    if weights_path.endswith('.weights'):  # Darknet format\n",
        "        model.load_state_dict(weights_path)\n",
        "    else:  # PyTorch format\n",
        "        checkpoint = torch.load(weights_path, map_location=device)\n",
        "        if 'model' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model'])\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "    \n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Model configuration\n",
        "    img_size = 416  # inference size (pixels)\n",
        "    conf_thres = 0.001  # object confidence threshold\n",
        "    iou_thres = 0.65  # IOU threshold for NMS\n",
        "    batch_size = 16  # batch size\n",
        "    \n",
        "    # Set paths\n",
        "    cfg_path = '/content/PyTorch_YOLOv4/cfg/yolov4.cfg'\n",
        "    weights_path = '/content/PyTorch_YOLOv4/weights/yolov4.weights'\n",
        "    \n",
        "    # Load model\n",
        "    print(\"Loading YOLOv4 model...\")\n",
        "    model = load_model(weights_path, cfg_path, img_size, device)\n",
        "    \n",
        "    # Create dataset and dataloader\n",
        "    print(\"Creating COCO dataset...\")\n",
        "    dataset = CocoDataset(\n",
        "        img_dir='/content/coco/val2017',\n",
        "        ann_file='/content/coco/annotations/instances_val2017.json',\n",
        "        img_size=img_size\n",
        "    )\n",
        "    \n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=lambda x: list(zip(*x))  # custom collate function\n",
        "    )\n",
        "    \n",
        "    \n",
        "    print(f\"\\nEvaluating YOLOv4 on {'subset of 100' if not DOWNLOAD_FULL_COCO else 'full'} COCO 2017 validation set...\")\n",
        "    print(f\"Settings: Image size={img_size}, Confidence threshold={conf_thres}, IoU threshold={iou_thres}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    stats = evaluate(model, dataloader, device, conf_thres, iou_thres)\n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nEvaluation completed in {elapsed:.2f} seconds\")\n",
        "    print(\"COCO mAP metrics:\")\n",
        "    print(f\"mAP@0.5:0.95 = {stats[0]:.5f}\")\n",
        "    print(f\"mAP@0.5 = {stats[1]:.5f}\")\n",
        "    print(f\"mAP@0.75 = {stats[2]:.5f}\")\n",
        "    \n",
        "    \n",
        "    print(\"\\nVisualizing some example detections...\")\n",
        "    visualize_examples(model, dataset, device, conf_thres=0.5, n=3)\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def visualize_examples(model, dataset, device, conf_thres=0.5, iou_thres=0.45, n=3):\n",
        "\n",
        "    import matplotlib.patches as patches\n",
        "    from matplotlib.colors import to_rgba\n",
        "    \n",
        "    # Select random images\n",
        "    indices = np.random.choice(len(dataset), n, replace=False)\n",
        "    \n",
        "    for idx in indices:\n",
        "        img_tensor, targets, img_path, (height, width) = dataset[idx]\n",
        "        \n",
        "        # Get original image for display\n",
        "        orig_img = cv2.imread(img_path)\n",
        "        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            img_batch = img_tensor.unsqueeze(0).to(device)\n",
        "            pred = model(img_batch)\n",
        "            pred = non_max_suppression(pred, conf_thres, iou_thres)[0]  # Predictions for this image\n",
        "        \n",
        "        \n",
        "        if pred is not None and len(pred) > 0:\n",
        "            pred[:, :4] = scale_coords(img_tensor.shape[1:], pred[:, :4], (height, width)).round()\n",
        "        \n",
        "        \n",
        "        fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "        ax.imshow(orig_img)\n",
        "        \n",
        "       \n",
        "        for t in targets:\n",
        "            label, x, y, w, h = t.tolist()\n",
        "            \n",
        "            # Convert normalized xywh to pixel xyxy\n",
        "            x1 = (x - w/2) * width\n",
        "            y1 = (y - h/2) * height\n",
        "            x2 = (x + w/2) * width\n",
        "            y2 = (y + h/2) * height\n",
        "            \n",
        "            # Create rectangle patch\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            \n",
        "            # Add class label\n",
        "            cls_name = dataset.class_names[int(label)]\n",
        "            plt.text(x1, y1-5, f'{cls_name}', color='lime', fontsize=11, bbox=dict(facecolor='black', alpha=0.5))\n",
        "        \n",
        "        # Draw predicted boxes\n",
        "        if pred is not None:\n",
        "            for *xyxy, conf, cls in pred:\n",
        "                x1, y1, x2, y2 = [x.item() for x in xyxy]\n",
        "                \n",
        "                # Create rectangle patch\n",
        "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "                \n",
        "                # Add class label with confidence\n",
        "                cls_name = dataset.class_names[int(cls)]\n",
        "                plt.text(x1, y1-5, f'{cls_name} {conf:.2f}', color='red', fontsize=11, bbox=dict(facecolor='black', alpha=0.5))\n",
        "        \n",
        "        # Set title and display\n",
        "        ax.set_title(f'Image: {os.path.basename(img_path)}')\n",
        "        ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
